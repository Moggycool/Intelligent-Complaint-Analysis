{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Exploratory Data Analysis and Data Preprocessing\n",
    "\n",
    "This notebook performs EDA on the CFPB complaint dataset and prepares it for the RAG pipeline.\n",
    "\n",
    "## Objectives:\n",
    "1. Load the full CFPB complaint dataset\n",
    "2. Perform initial EDA\n",
    "3. Filter dataset to specified products\n",
    "4. Clean text narratives\n",
    "5. Save cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = Path('../data/complaints.csv')\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(\"Dataset not found. Please run src/download_data.py first.\")\n",
    "    print(\"Or download manually from: https://www.consumerfinance.gov/data-research/consumer-complaints/\")\n",
    "else:\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(data_path, low_memory=False)\n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Distribution of Complaints Across Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get product distribution\n",
    "product_counts = df['Product'].value_counts()\n",
    "print(\"Top 20 Products by Complaint Count:\")\n",
    "print(product_counts.head(20))\n",
    "\n",
    "# Visualize top 20 products\n",
    "plt.figure(figsize=(12, 8))\n",
    "product_counts.head(20).plot(kind='barh')\n",
    "plt.xlabel('Number of Complaints')\n",
    "plt.ylabel('Product')\n",
    "plt.title('Top 20 Products by Complaint Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analysis of Consumer Complaint Narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count complaints with and without narratives\n",
    "total_complaints = len(df)\n",
    "complaints_with_narrative = df['Consumer complaint narrative'].notna().sum()\n",
    "complaints_without_narrative = df['Consumer complaint narrative'].isna().sum()\n",
    "\n",
    "print(f\"Total Complaints: {total_complaints:,}\")\n",
    "print(f\"Complaints WITH narratives: {complaints_with_narrative:,} ({complaints_with_narrative/total_complaints*100:.2f}%)\")\n",
    "print(f\"Complaints WITHOUT narratives: {complaints_without_narrative:,} ({complaints_without_narrative/total_complaints*100:.2f}%)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "labels = ['With Narrative', 'Without Narrative']\n",
    "sizes = [complaints_with_narrative, complaints_without_narrative]\n",
    "colors = ['#66b3ff', '#ff9999']\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Complaints With vs Without Narratives')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count for narratives\n",
    "def count_words(text):\n",
    "    \"\"\"Count words in a text string\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    return len(str(text).split())\n",
    "\n",
    "print(\"Calculating word counts for narratives...\")\n",
    "df['narrative_word_count'] = df['Consumer complaint narrative'].apply(count_words)\n",
    "\n",
    "# Filter to only complaints with narratives for analysis\n",
    "df_with_narrative = df[df['narrative_word_count'] > 0].copy()\n",
    "\n",
    "print(f\"\\nNarrative Word Count Statistics:\")\n",
    "print(df_with_narrative['narrative_word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word count distribution\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df_with_narrative['narrative_word_count'], bins=100, edgecolor='black')\n",
    "axes[0].set_xlabel('Word Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Narrative Word Counts')\n",
    "axes[0].axvline(df_with_narrative['narrative_word_count'].median(), \n",
    "                color='red', linestyle='--', label=f\"Median: {df_with_narrative['narrative_word_count'].median():.0f}\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Boxplot\n",
    "axes[1].boxplot(df_with_narrative['narrative_word_count'], vert=False)\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_title('Boxplot of Narrative Word Counts')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify very short and very long narratives\n",
    "very_short_threshold = 10  # words\n",
    "very_long_threshold = df_with_narrative['narrative_word_count'].quantile(0.95)\n",
    "\n",
    "very_short = df_with_narrative[df_with_narrative['narrative_word_count'] < very_short_threshold]\n",
    "very_long = df_with_narrative[df_with_narrative['narrative_word_count'] > very_long_threshold]\n",
    "\n",
    "print(f\"Very short narratives (< {very_short_threshold} words): {len(very_short):,} ({len(very_short)/len(df_with_narrative)*100:.2f}%)\")\n",
    "print(f\"Very long narratives (> {very_long_threshold:.0f} words): {len(very_long):,} ({len(very_long)/len(df_with_narrative)*100:.2f}%)\")\n",
    "print(f\"\\nExample of a very short narrative:\")\n",
    "if len(very_short) > 0:\n",
    "    print(very_short.iloc[0]['Consumer complaint narrative'])\n",
    "print(f\"\\nExample of a very long narrative (first 500 chars):\")\n",
    "if len(very_long) > 0:\n",
    "    print(str(very_long.iloc[0]['Consumer complaint narrative'])[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target products (Note: checking exact product names in dataset)\n",
    "# The problem mentions: Credit card, Personal loan, Savings account, Money transfers\n",
    "# We need to check actual product names in the dataset\n",
    "\n",
    "print(\"All unique products in dataset:\")\n",
    "all_products = df['Product'].unique()\n",
    "for i, product in enumerate(sorted(all_products), 1):\n",
    "    print(f\"{i}. {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to the specified products\n",
    "# Looking for: Credit card, Personal loan, Savings account, Money transfers\n",
    "target_products = [\n",
    "    'Credit card or prepaid card',\n",
    "    'Credit card',\n",
    "    'Prepaid card',\n",
    "    'Payday loan, title loan, or personal loan',\n",
    "    'Personal loan',\n",
    "    'Checking or savings account',\n",
    "    'Money transfer, virtual currency, or money service',\n",
    "    'Money transfers'\n",
    "]\n",
    "\n",
    "# Filter based on available products - let's be flexible\n",
    "# First, let's see what matches\n",
    "matching_products = [p for p in all_products if any(keyword in p.lower() for keyword in \n",
    "                     ['credit card', 'personal loan', 'payday loan', 'savings', 'checking', 'money transfer'])]\n",
    "\n",
    "print(\"\\nMatching products found:\")\n",
    "for product in matching_products:\n",
    "    count = len(df[df['Product'] == product])\n",
    "    print(f\"  {product}: {count:,} complaints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset\n",
    "print(\"Filtering dataset...\")\n",
    "print(f\"Original dataset size: {len(df):,}\")\n",
    "\n",
    "# Step 1: Filter by products\n",
    "df_filtered = df[df['Product'].isin(matching_products)].copy()\n",
    "print(f\"After product filtering: {len(df_filtered):,}\")\n",
    "\n",
    "# Step 2: Remove records with empty narratives\n",
    "df_filtered = df_filtered[df_filtered['Consumer complaint narrative'].notna()].copy()\n",
    "df_filtered = df_filtered[df_filtered['Consumer complaint narrative'].astype(str).str.strip() != ''].copy()\n",
    "print(f\"After removing empty narratives: {len(df_filtered):,}\")\n",
    "\n",
    "print(f\"\\nFiltered dataset shape: {df_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize filtered product distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_filtered['Product'].value_counts().plot(kind='barh')\n",
    "plt.xlabel('Number of Complaints')\n",
    "plt.ylabel('Product')\n",
    "plt.title('Filtered Dataset: Product Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean complaint narrative text:\n",
    "    - Convert to lowercase\n",
    "    - Remove special characters\n",
    "    - Remove boilerplate text\n",
    "    - Normalize whitespace\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove common boilerplate phrases\n",
    "    boilerplate_phrases = [\n",
    "        r'i am writing to file a complaint\\s*',\n",
    "        r'dear sir or madam\\s*',\n",
    "        r'to whom it may concern\\s*',\n",
    "        r'i am writing to\\s*',\n",
    "        r'xx+',  # Remove sequences of X's (often used for redaction)\n",
    "    ]\n",
    "    \n",
    "    for phrase in boilerplate_phrases:\n",
    "        text = re.sub(phrase, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^a-z0-9\\s.,!?\\-]', ' ', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the cleaning function\n",
    "sample_text = df_filtered.iloc[0]['Consumer complaint narrative']\n",
    "print(\"Original text (first 300 chars):\")\n",
    "print(sample_text[:300])\n",
    "print(\"\\nCleaned text (first 300 chars):\")\n",
    "print(clean_text(sample_text)[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to all narratives\n",
    "print(\"Cleaning narratives...\")\n",
    "df_filtered['cleaned_narrative'] = df_filtered['Consumer complaint narrative'].apply(clean_text)\n",
    "\n",
    "# Remove any that became empty after cleaning\n",
    "df_filtered = df_filtered[df_filtered['cleaned_narrative'].str.len() > 0].copy()\n",
    "print(f\"After cleaning and removing empty: {len(df_filtered):,}\")\n",
    "\n",
    "# Recalculate word count for cleaned narratives\n",
    "df_filtered['cleaned_word_count'] = df_filtered['cleaned_narrative'].apply(count_words)\n",
    "\n",
    "print(\"\\nCleaned Narrative Statistics:\")\n",
    "print(df_filtered['cleaned_word_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for final dataset\n",
    "columns_to_save = [\n",
    "    'Complaint ID',\n",
    "    'Product',\n",
    "    'Sub-product',\n",
    "    'Issue',\n",
    "    'Sub-issue',\n",
    "    'Consumer complaint narrative',\n",
    "    'cleaned_narrative',\n",
    "    'Company',\n",
    "    'State',\n",
    "    'Date received',\n",
    "    'Date sent to company'\n",
    "]\n",
    "\n",
    "# Filter to only columns that exist\n",
    "available_columns = [col for col in columns_to_save if col in df_filtered.columns]\n",
    "df_final = df_filtered[available_columns].copy()\n",
    "\n",
    "# Save to CSV\n",
    "output_path = Path('../data/filtered_complaints.csv')\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {output_path}\")\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(f\"\\nColumns saved: {list(df_final.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total complaints in cleaned dataset: {len(df_final):,}\")\n",
    "print(f\"\\nProduct distribution:\")\n",
    "print(df_final['Product'].value_counts())\n",
    "print(f\"\\nNarrative length statistics (cleaned):\")\n",
    "print(df_filtered['cleaned_word_count'].describe())\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Dataset Overview**: The CFPB complaint dataset contains a large number of consumer complaints across various financial products. A significant portion of complaints include detailed narratives describing the issues.\n",
    "\n",
    "2. **Narrative Analysis**: \n",
    "   - The dataset shows considerable variation in narrative length, with some complaints being very brief (under 10 words) and others being quite detailed (over several hundred words).\n",
    "   - The median narrative length provides a good indicator of typical complaint detail level.\n",
    "   - Not all complaints include narratives, which is why filtering for non-empty narratives is crucial for our RAG pipeline.\n",
    "\n",
    "3. **Product Focus**: By filtering to the specified product categories (credit cards, personal loans, savings accounts, and money transfers), we've created a focused dataset that aligns with the project requirements. The filtered dataset maintains a good distribution across these product categories, ensuring our RAG system will have diverse examples to work with.\n",
    "\n",
    "4. **Text Cleaning**: The cleaning process successfully normalized the text by:\n",
    "   - Converting all text to lowercase for consistency\n",
    "   - Removing boilerplate phrases that don't add semantic value\n",
    "   - Eliminating special characters and URLs that could interfere with embedding quality\n",
    "   - Preserving the core complaint content while improving text quality for downstream processing\n",
    "\n",
    "The cleaned dataset is now ready for text chunking, embedding, and vector store indexing in Task 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}